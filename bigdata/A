Q1(a)
Explain the architecture of HDFS and describe the roles of the NameNode and DataNode.
Answer:
HDFS Architecture (Master–Slave):
• HDFS = Hadoop Distributed File System; used for reliable, scalable distributed
storage.
• Follows master–slave architecture:
o One active NameNode (master)
o Many DataNodes (slaves)
NameNode (Master):
• Stores metadata (file names, hierarchy, permissions, block locations).
• Manages file system namespace (directory tree).
• Coordinates operations like create, delete, rename files/directories.
• Does not store actual file data, only metadata in memory.
DataNodes (Slaves):
• Store actual data blocks on local disks.
• Serve read/write requests from clients and NameNode.
• Periodically send:
o Heartbeats (to confirm they’re alive)
o Block reports (which blocks they hold)
Data Storage Workflow:
• File is split into blocks (e.g., 128 MB).
• Each block is replicated across multiple DataNodes (default factor 3).
• NameNode tracks which DataNode has which block

Q1(b)
What is HBase? Compare it with traditional RDBMS and mention two use cases.
Answer:
What is HBase?
• HBase is a NoSQL, distributed, column-family store built on top of HDFS, inspired by
Google Bigtable.
• Designed for random real-time read/write access to big data.

HBase vs RDBMS:

Aspect    HBase (NoSQL)                        Traditional RDBMS
Schema    Schema-flexible (column families)    Fixed schema (tables with fixed
columns)
Data Model Column family, sparse, wide tables   Relational (rows/columns,normalized)
Joins     Not natively supported                Supported via SQL
Scaling   Horizontal scale over commodityservers       Mostly vertical; scale-up
Transactions     Limited atomicity (row-level)         ACID transactions
Query            Language No SQL; uses APIs & filters           SQL


Two Use Cases:
1. Time-series / log data storage
o Clickstreams, sensor data, server logs where data is large and append-heavy.
2. Real-time lookup for large datasets
o User profiles, recommendation data, key-based access with low latency

Q1(c)
What is YARN in Hadoop? Explain its function and list its key components.
Answer:
YARN (Yet Another Resource Negotiator):
• Acts as cluster resource management and job scheduling layer in Hadoop.
• Decouples resource management from data processing models, allowing different
engines (MapReduce, Spark, etc.) to run on the same cluster.

Main Functions:
• Track available resources (CPU, memory) across nodes.
• Allocate resources to applications.
• Schedule tasks and monitor application lifecycle.
Key Components:
1. ResourceManager (RM):
o Central authority for resource allocation.
o Runs Scheduler (allocates containers) and ApplicationManager.
2. NodeManager (NM):
o Runs on each worker node.
o Manages containers.
o Reports resource usage and health to RM.
3. ApplicationMaster (AM):
o One per application/job.
o Negotiates resources from RM.
o Coordinates tasks’ execution inside containers on NMs.
4. Containers:
o Resource bundles (CPU + RAM) granted to an AM or task.
o Execution environment for individual tasks.



Q1(d)
What is the role of DAG (Directed Acyclic Graph) in Apache Spark? How does it improve job
execution compared to MapReduce?
Answer:
Role of DAG in Spark:
• Spark builds a DAG of stages and tasks for each job.
• Nodes = RDD/DataFrame operations, edges = dependencies.
• The DAG describes how data flows through transformations.
How it improves over MapReduce:
1. Global Optimization:
o Spark’s DAG scheduler can optimize across multiple operations (e.g.,
pipelining, collapsing stages).
o MapReduce treats each job independently (map → shuffle → reduce).

2. In-Memory Computation:
o Intermediate results can be cached in memory (RDD/DataFrame cache).
o MapReduce writes intermediates to disk between every phase.
3. Less Disk I/O & Latency:
o Fewer read/write cycles → faster execution.
o Especially beneficial for iterative algorithms and interactive queries.
4. Pipelining:
o Multiple narrow transformations can run in a single stage.
o MapReduce has fixed map/reduce stages with disk in-between.



Q1(a)
What is Hadoop? Describe the role of Distributed Computing Environment and list two
components of the Hadoop Ecosystem.
Answer:
• Hadoop is an open-source framework for distributed storage (HDFS) and parallel
processing (MapReduce/YARN) of large datasets across clusters of commodity
hardware.
Role of Distributed Computing Environment:
• Splits data into blocks and stores across multiple nodes.
• Moves computation to the data (data locality).
• Enables parallel processing of big data.
• Provides fault tolerance through replication and re-execution.
• Allows horizontal scalability by adding more nodes.
Two Hadoop Ecosystem Components:
1. HDFS – Distributed storage system.
2. YARN or MapReduce – Resource management & proces


Q1(b)
Describe the architecture of Hive. Mention the role of Metastore, Driver, Compiler, and
Execution Engine.
Answer:
• Hive provides SQL-like querying (HiveQL) over data stored in HDFS.
• Main components: User Interface, Driver, Compiler, Optimizer, Execution Engine,
Metastore.
Metastore:
• Stores metadata about databases, tables, columns, partitions, and file locations.
• Backed by traditional RDBMS.
• Used by Compiler for semantic analysis.
Driver:
• Manages the life cycle of a Hive query.
• Receives query, coordinates compilation, optimization and execution.
• Maintains session and returns results to the client.
Compiler:
• Parses HiveQL.
• Performs semantic analysis (validity of tables/columns, types).
• Generates logical plan → physical plan (a set of stages, often MapReduce/Tez/Spark
jobs).
Execution Engine:
• Submits the physical plan as jobs to the underlying Hadoop/YARN.
• Monitors execution, handles failures.
• Collects results and hands back to the Driver


Q1(c)
What is the CAP Theorem? Explain how it relates to NoSQL database types and name the
four main types of NoSQL databases.
Answer:
CAP Theorem:
In a distributed system, you can only fully guarantee two of these three:
• Consistency (C): All nodes see the same data at the same time.
• Availability (A): Every request receives a non-error response, regardless of node
failures.
• Partition Tolerance (P): System continues to function despite network partitions.
In presence of partitions (P), systems choose between C and A:
• CP systems: Prefer consistency over availability (e.g., HBase).
• AP systems: Prefer availability over strong consistency (e.g., Cassandra).
Relation to NoSQL Types:
Most NoSQL systems are distributed → must handle P, so they trade between C and A.
Different NoSQL databases (key–value, column-family, document, graph) pick CAP trade-offs
based on use case.
Four Main Types of NoSQL Databases:
1. Key–Value Stores (e.g., Redis, Riak)
2. Document Stores (e.g., MongoDB, CouchDB)
3. Column-Family Stores (e.g., HBase, Cassandra)
4. Graph Databases (e.g., Neo4j)


Q1(d)
Apache Spark is often described as a unified analytics engine. Explain this statement in
detail and contrast Spark’s in-memory computation model with Hadoop MapReduce’s
disk-based processing.
Answer:
Unified Analytics Engine – Why:
• Spark supports multiple workloads on one engine:
o Batch (Spark Core)
o SQL (Spark SQL)
o Streaming (Structured Streaming)
o ML (MLlib)
o Graph (GraphX)
• Uses unified APIs (RDDs, DataFrames, Datasets).
• Works with various storage systems: HDFS, S3, Kafka, HBase, etc.
Spark In-Memory vs MapReduce Disk-Based:
• Spark:
o Keeps data in memory between transformations (RDD/DataFrame caching).
o Uses lazy evaluation and builds a DAG for optimization.
o Ideal for iterative algorithms and interactive analysis.
o Significantly reduces disk I/O.
• MapReduce:
o Each job has fixed map → shuffle → reduce stages.
o Intermediate results are written to disk (HDFS) after each stage.
o Higher latency and overhead, especially for multi-step and iterative jobs.
Result:
Spark is often 10–100× faster in memory and several times faster than MapReduce on diskheavy workloads.

Q1(a)
List two primary differences between Hadoop version 2 and Hadoop version 3.
Answer (any two valid):
1. Erasure Coding:
o Hadoop 2: Only replication for fault tolerance (e.g., 3x).
o Hadoop 3: Introduces erasure coding → same reliability with less storage
overhead.
2. Multiple Standby NameNodes:
o Hadoop 2: Typically 1 active + 1 standby NameNode (limited HA).
o Hadoop 3: Supports multiple standby NameNodes, improving availability.
Other acceptable points:
• YARN improvements (opportunistic containers, etc.).
• Support for heterogeneous storage (SSD/HDD tiers).
• MapReduce 2 vs improvements in container reuse & scheduling.

Q1(b)
What is Hive metastore? Can NoSQL database HBase be configured as Hive metastore?
(3+1)
Answer:
Hive Metastore (3 marks):
• Central repository holding metadata for Hive:
o Databases, tables, columns, partitions, data types.
o Table location in HDFS and statistics.
• Usually implemented as an RDBMS (e.g., MySQL, Postgres).
• Allows Hive to treat HDFS files as structured, queryable tables.
Can HBase be used as Hive Metastore? (1 mark)
• No. HBase is a NoSQL column-family store, not a relational database.
• Hive Metastore requires relational features and transactions; hence HBase is not used
as the metastore backend.

Q1(c)
Using an example, depict how MapReduce computes word count.
Answer:
Goal: Count occurrences of each word in a text corpus.
Input Example: 2 lines
• L1: "hello world"
• L2: "hello hadoop"
Map Phase:
Mapper processes each line and emits (word, 1):
• For L1:
o ("hello", 1)
o ("world", 1)
• For L2:
o ("hello", 1)
o ("hadoop", 1)
Shuffle & Sort Phase:
• Group by key (word):
• "hello" → [1, 1]
• "world" → [1]
• "hadoop" → [1]
Reduce Phase:
Reducer sums the values per key:
• "hello" → 1 + 1 = 2 → output: ("hello", 2)
• "world" → 1 → ("world", 1)
• "hadoop" → 1 → ("hadoop", 1)
Final Output:
Plain Text
hello 2
world 1
hadoop 1
Show more lines


Q1(d)
Draw Spark architecture and explain its various components.
Answer (description):
Spark follows a cluster architecture with:
• Driver Program
• Cluster Manager (Standalone / YARN / Mesos / Kubernetes)
• Worker Nodes
• Executors
Components:
1. Driver Program:
o Runs the main() of the application.
o Creates SparkContext / SparkSession.
o Defines transformations and actions on RDDs/DataFrames.
o Coordinates execution and collects results.
2. Cluster Manager:
o Allocates resources for Spark applications.
o Examples: Standalone, YARN, Mesos, Kubernetes.
3. Workers (Cluster Nodes):
o Machines where the tasks actually run.
o Host executors.
4. Executors:
o JVM processes launched on worker nodes.
o Execute tasks and store data in memory or on disk.
o One application usually has multiple executors.
5. Tasks:
o Small units of work scheduled on executors.
Spark’s DAG Scheduler splits a job into stages and tasks assigned to executors.

Q1(e)
What is CAP theorem? Where does MongoDB stand in CAP theorem? (3+1)
Answer:
• CAP Theorem (3 marks):
In a distributed system, you can only guarantee two out of three:
o Consistency (C)
o Availability (A)
o Partition Tolerance (P)
Under network partition, the system must choose either C or A.
• MongoDB (1 mark):
MongoDB is generally considered an AP (Available + Partition Tolerant) system by
default using replica sets with eventual consistency.
However, using write concerns/read preferences, it can trade closer to CP for some
operations. For exam answers: AP is acceptable.

Q1(a)
Draw Spark architecture and explain its various components. (6 marks)
(Same conceptual answer as earlier Spark architecture question; can reuse with more detail if
needed.)
Answer (summary):
• Driver Program – Runs application, creates SparkSession, defines
transformations/actions, communicates with Cluster Manager.
• Cluster Manager – Allocates resources (Standalone, YARN, Mesos, Kubernetes).
• Worker Nodes – Machines that run executors.
• Executors – JVMs on workers; execute tasks and keep data in memory.
• Tasks – Units of work scheduled on executors.
• DAG Scheduler – Splits job into stages based on dependencies; Task Scheduler assigns
tasks to executor

Q1(b)
List any 4 differences between Data Lake and Data Warehouse. (4 marks)
Answer (any 4):
1. Data Type:
o Data Lake: Stores raw, unstructured, semi-structured, structured.
o Data Warehouse: Stores processed, structured data.
2. Schema:
o Data Lake: Schema-on-read.
o Data Warehouse: Schema-on-write.
3. Purpose:
o Data Lake: For data exploration, ML, big data analytics.
o Data Warehouse: For BI, reporting, OLAP.
4. Users:
o Data Lake: Data scientists, data engineers.
o Data Warehouse: Business analysts, decision-makers.
5. Cost & Storage:
o Data Lake: Cheaper storage; often on distributed storage.
o Data Warehouse: More expensive, performance-optimized

Q1(c)
Explain partitioning in Hive with an example. (6 marks)
Answer:
• Partitioning: Technique to divide a Hive table into subdirectories based on partition
column(s).
• Improves performance by scanning only relevant partitions instead of the entire table.
Example:
Partition a sales table by year and month:
SQL
CREATE TABLE sales (
sale_id INT,
amount DOUBLE,
item STRING
)
PARTITIONED BY (year INT, month INT)
STORED AS TEXTFILE;
``
Show more lines
Data is stored like:
• /warehouse/sales/year=2024/month=1/
• /warehouse/sales/year=2024/month=2/
Query scanning only year=2024 AND month=1 will read only that partition directory.

Q1(e)
List any 2 differences between Coalesce and Repartition in Spark. (4 marks)
Answer:
1. Shuffling:
o coalesce() reduces the number of partitions, often without full shuffle (unless
shuffle=True).
o repartition() can increase or decrease partitions but always performs a full
shuffle.
2. Use Case:
o coalesce(n) is efficient when reducing partitions (e.g., from 100 to 10).
o repartition(n) is used when increasing partitions or for more uniform data
distribution

Q1(a)
Write a note on Spark RDD and DAG.
Answer:
• RDD (Resilient Distributed Dataset):
o Core data structure in Spark.
o Immutable, distributed collection of objects partitioned across cluster.
o Supports transformations (map, filter, reduceByKey) and actions (collect,
count).
o Resilient: Can recompute lost partitions using lineage.
• DAG (Directed Acyclic Graph):
o When applying transformations on RDDs, Spark builds a DAG of operations.
o Nodes = RDDs/operations, edges = dependencies.
o DAG scheduler optimizes execution (grouping stages, minimizing shuffles).

Q1(d)
Explain MapReduce using PyMongo with suitable code snippet.
Answer (idea):
MongoDB supports MapReduce-like operations; in PyMongo you can define map and reduce
JavaScript functions as strings and call map_reduce on a collection.
Example (word count on a texts collection):
Python
from pymongo import MongoClient
client = MongoClient("mongodb://localhost:27017/")
db = client["mydb"]
coll = db["texts"]
# Map function (JavaScript)
map_func = """
function() {
var words = this.text.split(" ");
for (var i = 0; i < words.length; i++) {
emit(words[i], 1);
}
}
"""
# Reduce function (JavaScript)
reduce_func = """
function(key, values) {
return Array.sum(values);
}
"""
result = coll.map_reduce(map_func, reduce_func, "word_counts")
for doc in result.find():
print(doc["_id"], doc["value"])
``
Show more lines

Q1(e)
How to create bucketing table in Hive, write an example query for it. List the benefits of
bucketing.
Answer:
Example bucketing table:
SQL
CREATE TABLE employee_bucketed (
emp_id INT,
name STRING,
dept STRING
)
CLUSTERED BY (emp_id) INTO 4 BUCKETS
STORED AS ORC;
``
Show more lines
Benefits of Bucketing:
• Enables more efficient sampling.
• Helps with join optimization (bucketed tables on same key).
• Provides more evenly distributed data within files.
• Reduces the amount of data scanned for certain queries





Section B 

Q2(a) – HDFS Shell Commands (10 marks)
1. Remove file olddata.csv from /user/hadoop/backup/
Shell
hdfs dfs -rm /user/hadoop/backup/olddata.csv
``
Show more lines
2. Create nested directory /user/hadoop/project/data/
Shell
hdfs dfs -mkdir -p /user/hadoop/project/data/
Show more lines
3. Display first 10 lines of /user/hadoop/reports/report.txt
Shell
hdfs dfs -cat /user/hadoop/reports/report.txt | head -n 10
Show more lines
(If head not desired, some environments also support hdfs dfs -tail for end of file only.)
4. Recursively copy local Logs/ into HDFS /user/hadoop/logs_backup/
Shell
hdfs dfs -copyFromLocal -R Logs/ /user/hadoop/logs_backup/
Show more lines
5. Check disk usage of /user/hadoop/data/
Shell
hdfs dfs -du -h /user/hadoop/data/
``
Show more lines

Q2(b) – Spark RDD & PySpark (10 marks)
1. Create RDD of key–value pairs and group by key
Python
rdd = sc.parallelize([
("math", 85),
("science", 90),
("math", 75),
("english", 70)
])
grouped = rdd.groupByKey()
# To view as (key, list_of_values):
grouped_result = grouped.mapValues(list).collect()
``
Show more lines
2. From RDD of lines, extract all words and remove duplicates
Assume:
Python
lines_rdd = sc.textFile("path/to/file.txt")
``
Show more lines
Then:
Python
words_unique = (lines_rdd
.flatMap(lambda line: line.split())
.distinct())
``
Show more lines
3. Given an RDD of integers, compute sum
Assume:
Python
int_rdd = sc.parallelize([1, 2, 3, 4])
``
Show more lines
Then:
Python
total_sum = int_rdd.reduce(lambda x, y: x + y)
Show more lines
4. PySpark code: Read JSON → select name, salary → filter salary > 50000 → count
Python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
spark = SparkSession.builder.appName("HighSalaryCount").getOrCreate()
# 1) Read JSON file
df = spark.read.json("path/to/input.json")
# 2) Select only name and salary
df_selected = df.select("name", "salary")
# 3) Filter salary > 50000
df_filtered = df_selected.filter(col("salary") > 50000)
# 4) Count such rows
count_high_salary = df_filtered.count()
print("Number of records with salary > 50000:", count_high_salary)
Show more lines


Q2(c) – Hive Queries (10 marks)
1. Create partitioned external table analytics_db.sales_data partitioned by region
SQL
CREATE EXTERNAL TABLE analytics_db.sales_data (
sale_id INT,
item STRING,
amount DOUBLE,
sale_date STRING
)
PARTITIONED BY (region STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/hive/warehouse/sales_data';
Show more lines
2. Right outer join orders and customers on customer_id with amount > 500
SQL
SELECT *
FROM orders o
RIGHT OUTER JOIN customers c
ON o.customer_id = c.customer_id
WHERE o.amount > 500;
Show more lines
3. Second highest salary from employee_details
SQL
SELECT MAX(salary)
FROM employee_details
WHERE salary < (SELECT MAX(salary) FROM employee_details);
Show more lines

Q2(d) – MongoDB (10 marks)
1. Insert 4 documents into product_inventory
JavaScript
db.product_inventory.insertMany([
{ product_name: "Laptop", category: "Electronics", price: 60000, stock_quantity: 25 },
{ product_name: "Mouse", category: "Accessories", price: 1000, stock_quantity: 40 },
{ product_name: "TV", category: "Electronics", price: 45000, stock_quantity: 12 },
{ product_name: "Charger", category: "Accessories", price: 1500, stock_quantity: 20 }
])
Show more lines
2. Find all Electronics with price > 10000
JavaScript
db.product_inventory.find({
category: "Electronics",
price: { $gt: 10000 }
})
Show more lines
3. Update Accessories: reduce stock_quantity by 10 and return updated documents
JavaScript
db.product_inventory.updateMany(
{ category: "Accessories" },
{ $inc: { stock_quantity: -10 } }
)
db.product_inventory.find({ category: "Accessories" })
Show more lines
4. Delete products where stock_quantity < 15
JavaScript
db.product_inventory.deleteMany({
stock_quantity: { $lt: 15 }
})



Q2(a) – HDFS Shell Commands (10 marks)
1. Create directory InputDir under /user/hadoop/
Shell
hdfs dfs -mkdir /user/hadoop/InputDir
Show more lines
2. List contents of SampleDir with sizes and times
Shell
hdfs dfs -ls SampleDir
``
Show more lines
3. Upload all .txt from local XYZ to /user/hadoop/InputDir/
Shell
hdfs dfs -put XYZ/*.txt /user/hadoop/InputDir/
Show more lines
4. Read file.txt from HDFS and view with less
Assuming it’s in /user/hadoop/InputDir/file.txt:
Shell
hdfs dfs -cat /user/hadoop/InputDir/file.txt | less
Show more lines
5. Move OutputDir to ArchiveDir within HDFS
Shell
hdfs dfs -mv /user/hadoop/OutputDir /user/hadoop/ArchiveDir/
Show more lines

Q2(b) – Spark RDD & PySpark (10 marks)
1. Create an RDD from list of strings
Python
rdd = sc.parallelize(["apple", "banana", "orange", "grape"])
Show more lines
2. Count number of elements in RDD
Python
count = rdd.count()
Show more lines
3. Filter even numbers from an RDD of numbers
Assume:
Python
nums_rdd = sc.parallelize([1, 2, 3, 4, 5, 6])
Show more lines
Then:
Python
even_rdd = nums_rdd.filter(lambda x: x % 2 == 0)
Show more lines
4. Read text file and count lines containing “error”
Python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ErrorCount").getOrCreate()
lines_rdd = spark.sparkContext.textFile("path/to/logfile.txt")
error_lines = lines_rdd.filter(lambda line: "error" in line.lower())
error_count = error_lines.count()
print("Number of lines containing 'error':", error_count)

Q2(c) – Hive Queries (10 marks)
1. Create external table products in sales_db
SQL
USE sales_db;
CREATE EXTERNAL TABLE products (
product_id INT,
product_name STRING,
category STRING,
price FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/hive/products';
Show more lines
2. Left outer join employees and departments on dept_id
SQL
SELECT e.*, d.*
FROM employees e
LEFT OUTER JOIN departments d
ON e.dept_id = d.dept_id;
Show more lines
3. Top 5 highest-paid employees from employee_details
SQL
SELECT *
FROM employee_details
ORDER BY salary DESC
LIMIT 5;

Q2(d) – MongoDB (10 marks)
1. Create customer_data collection
JavaScript
db.createCollection("customer_data")
Show more lines
2. Insert 4 documents (name, email, city)
JavaScript
db.customer_data.insertMany([
{ name: "Alice", email: "alice@example.com", city: "Delhi" },
{ name: "Bob", email: "bob@example.com", city: "Mumbai" },
{ name: "John", email: "john@example.com", city: "Bombay" },
{ name: "Sara", email: "sara@example.com", city: "Delhi" }
])
Show more lines

3. Find customers from Delhi
JavaScript
db.customer_data.find({ city: "Delhi" })
Show more lines

4. Update city “Bombay” → “Mumbai” and display modified docs
JavaScript
db.customer_data.updateMany(
{ city: "Bombay" },
{ $set: { city: "Mumbai" } }
)
db.customer_data.find({ city: "Mumbai" })

5. Delete documents where name is “John”
JavaScript
db.customer_data.deleteMany({ name: "John" })

Q2(a) – HDFS Shell Commands (14 marks)
i. Print version of installed Hadoop (2 marks)
Shell
hadoop version
``
Show more lines
ii. Copy file1.txt from InputDir to OutputDir as file2.txt (3 marks)
Shell
hdfs dfs -cp InputDir/file1.txt OutputDir/file2.txt
Show more lines
iii. Delete empty directory XYZ (3 marks)
Shell
hdfs dfs -rmdir XYZ
Show more lines
iv. List contents of SampleDir (3 marks)
Shell
hdfs dfs -ls SampleDir
``
Show more lines
v. Fetch usage/help for mkdir command (3 marks)
Shell
hdfs dfs -help mkdir

Q2(b) – Spark word count pseudo-code (8 marks)
Pseudo-code to load text.txt into an RDD and compute word count:
Python
from pyspark import SparkContext
sc = SparkContext("local", "WordCount")
# 1) Load text file into RDD
text_rdd = sc.textFile("text.txt")
# 2) Split lines into words
words_rdd = text_rdd.flatMap(lambda line: line.split())
# 3) Map each word to (word, 1)
pair_rdd = words_rdd.map(lambda w: (w, 1))
# 4) Reduce by key to sum counts
word_counts = pair_rdd.reduceByKey(lambda a, b: a + b)
# 5) Collect or save result
result = word_counts.collect()
for (word, count) in result:
print(word, count)

Q2(c) – Hive Join + Expected Output (8 marks)
Tables:
Table1
Name Id
Joe 2
Hank 4
Ali 0
Table2
Id Name
2 Tie
4 Coat
3 Hat
1 Scarf
Inner join on Id:
SQL
SELECT t1.Name AS name1,
t1.Id,
t2.Name AS name2
FROM Table1 t1
INNER JOIN Table2 t2
ON t1.Id = t2.Id;
Show more lines
Expected Output:
Only matching Ids are 2 and 4:
name1 Id name2
Joe 2 Tie
Hank 4 Coat


Q2(d) – MongoDB (10 marks)
i. Create collection orders (1 mark)
JavaScript
db.createCollection("orders")
Show more lines
ii. Insert record into orders (3 marks)
JavaScript
db.orders.insertOne({
"order_id": 1,
"order_date": "2013-07-25 00:00:00.0",
"order_customer_id": 11599,
"order_status": "CLOSED"
})
``
Show more lines
iii. Fetch orders with order_status = "COMPLETE" (3 marks)
JavaScript
db.orders.find({ order_status: "COMPLETE" })
Show more lines
iv. Compute count of orders with status COMPLETE and CLOSED (3 marks)
JavaScript
db.orders.aggregate([
{ $match: { order_status: { $in: ["COMPLETE", "CLOSED"] } } },
{ $group: { _id: "$order_status", count: { $sum: 1 } } }
])

Q2(a) – HDFS Commands (10 marks)
Assume all paths are relative to current HDFS directory.
1. Copy file1.txt from InputDir to OutputDir as file2.txt
Shell
hdfs dfs -cp InputDir/file1.txt OutputDir/file2.txt
Show more lines
2. Delete empty directory XYZ
Shell
hdfs dfs -rmdir XYZ
Show more lines
3. List files & directories under SampleDir
Shell
hdfs dfs -ls SampleDir
Show more lines
4. Recursively list files & directories under SampleDir
Shell
hdfs dfs -ls -R SampleDir
``
Show more lines
5. Change permission of file.txt to read-only (444)
Shell
hdfs dfs -chmod 444 file.txt
Show more lines

Q2(b) – Spark RDD (10 marks)
1. Create an RDD from List(1, 2, 3, 4, 5, 6)
Python
rdd = sc.parallelize([1, 2, 3, 4, 5, 6])
Show more lines
2. Read/load text file from /path/to/file.txt into RDD
Python
text_rdd = sc.textFile("/path/to/file.txt")
Show more lines
3. Filter even numbers from RDD
Python
even_rdd = rdd.filter(lambda x: x % 2 == 0)
Show more lines
4. Map each element to its square
Python
squared_rdd = rdd.map(lambda x: x * x)
``
Show more lines
5. Count number of elements
Python
count = rdd.count()

Q2(c) – Hive (10 marks)
1. Create database anotherDB
SQL
CREATE DATABASE anotherDB;
``
Show more lines
2. Create EXTERNAL TABLE orders1 in anotherDB
SQL
USE anotherDB;
CREATE EXTERNAL TABLE orders1 (
order_id INT,
order_date STRING,
order_customer_id INT,
order_status STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/hive/warehouse/anotherDB.db/orders1';
``
Show more lines
(Location can vary; main idea is EXTERNAL and columns are correct.)
3. Load data into orders1 from local file
Assuming local file path /local/path/orders1.csv:
SQL
LOAD DATA LOCAL INPATH '/local/path/orders1.csv'
INTO TABLE orders1;

Q2(d) – MongoDB (10 marks)
1. Create orders collection
JavaScript
db.createCollection("orders")
``
Show more lines
2. Insert two records into orders
JavaScript
db.orders.insertMany([
{ "order_id": 1, "order_customer_id": 11599, "order_status": "CLOSED" },
{ "order_id": 2, "order_customer_id": 11698, "order_status": "OPEN" }
])
Show more lines
3. Fetch orders with status COMPLETE
JavaScript
db.orders.find({ order_status: "COMPLETE" })

Q2(a) – HDFS Commands (8 marks)
1. Print Hadoop version
Shell
hadoop version
Show more lines
2. Syntax to list files/dirs in HDFS path /path
Shell
hdfs dfs -ls /path
Show more lines
3. Syntax to copy local file1 to HDFS
Shell
hdfs dfs -copyFromLocal file1 /path/in/hdfs/
Show more lines
(or -put)
4. Copy testfile from HDFS to local
Shell
hdfs dfs -copyToLocal /path/in/hdfs/testfile /local/path/

5. Display content of sample file inside newDataFlair
Shell
hdfs dfs -cat newDataFlair/sample

Q2(b) – Spark RDD Commands (10 marks)
Given: list1 = [10, 20, 30, 40, 50]
Python
rdd = sc.parallelize([10, 20, 30, 40, 50])
# 1. Display all elements
print(rdd.collect())
# 2. Sum of all elements
print(rdd.sum())
# 3. Maximum
print(rdd.max())
# 4. Minimum
print(rdd.min())
# 5. Mean
print(rdd.mean())
# 6. Standard deviation
print(rdd.stdev())
# 7. Variance
print(rdd.variance())

# 8. First element
print(rdd.first())
# 9. Number of elements
print(rdd.count())
Show more lines
Q2(c) – Hive external table stocks (4 marks)
SQL
CREATE EXTERNAL TABLE stocks (
exch STRING,
stock_symbol STRING,
stock_date STRING,
open_price FLOAT,
high_price FLOAT,
low_price FLOAT,
close_price FLOAT,
volume INT,
adj_close_price FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/hadoop/StocksDir/';

Q2(d) – MongoDB (8 marks)
1. Create users collection
JavaScript
db.createCollection("users")
Show more lines
2. Insert 5 records with name and age
JavaScript
db.users.insertMany([
{ name: "Amit", age: 30 },
{ name: "Rahul", age: 35 },
{ name: "Sneha", age: 40 },
{ name: "John", age: 34 },
{ name: "Priya", age: 28 }
])
``
Show more lines
3. Display users with age >= 34
JavaScript
db.users.find({ age: { $gte: 34 } })


--------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


# Spark session
spark = SparkSession.builder.appName("SparkML").getOrCreate()

# Load data
df = spark.read.option("header", "true").option("inferSchema", "true").csv("bank_data.csv")

# String columns
string_cols = ["job", "marital", "education", "loan", "deposit"]

# StringIndexers (IMPORTANT FIX)
indexers = [
 StringIndexer(
 inputCol=col,
 outputCol=col + "_index",
 handleInvalid="keep"
 )
 for col in string_cols
]


# Feature columns
feature_cols = [
 "age",
 "balance",
 "job_index",
 "marital_index",
 "education_index",
 "loan_index"
]

     

# VectorAssembler
assembler = VectorAssembler(
 inputCols=feature_cols,
 outputCol="features"
)


# Logistic Regression model
lr = LogisticRegression(
 featuresCol="features",
 labelCol="deposit_index"
)


# Pipeline
pipeline = Pipeline(stages=indexers + [assembler, lr])


# Train-test split
train_data, test_data = df.randomSplit([0.75, 0.25], seed=42)


# Train model
model = pipeline.fit(train_data)


# Predictions
predictions = model.transform(test_data)


# Evaluation
evaluator = MulticlassClassificationEvaluator(
 labelCol="deposit_index",
 predictionCol="prediction",
 metricName="accuracy"
)
accuracy = evaluator.evaluate(predictions)
print("Accuracy:", accuracy)

from pyspark.sql import SparkSession
from pyspark.sql.functions import min, max, avg
# 1️⃣ Create Spark Session
spark = SparkSession.builder.appName("SectionC_PartA").getOrCreate()
# 2️⃣ Load Dataset
df = spark.read.option("header", "true").option("inferSchema", "true").csv("bank_data.csv")
# Verify data
df.show()
df.printSchema()
# ---------------------------------------
# Q1. Minimum, Maximum, Average balance
# ---------------------------------------
df.select(min("balance").alias("min_balance"), max("balance").alias("max_balance"), avg("balance").alias("avg_balance")).show()
# ---------------------------------------
# Q2. Job type with highest number of people
# ---------------------------------------
df.groupBy("job").count().orderBy("count", ascending=False).show()
# Only top job
df.groupBy("job").count().orderBy("count", ascending=False).limit(1).show()
# ---------------------------------------
# Q3. Count people aged over 60 AND having a loan
# ---------------------------------------
df.filter((df.age > 60) & (df.loan == "yes")).count()
# ---------------------------------------
# Q4. Count deposits ('yes') by education level
# ---------------------------------------
df.filter(df.deposit == "yes").groupBy("education").count().show()
# ---------------------------------------
# Q5. Deposit count ('yes' vs 'no') for each marital status
# ---------------------------------------
df.groupBy("marital", "deposit").count().show()
# ---------------------------------------
# Q6. Remove all rows containing ANY null values
# ---------------------------------------
df_clean = df.dropna()
df_clean.show()
# ---------------------------------------
# EDGE CASE 1: Balance greater than 1000
# ---------------------------------------
df.filter(df.balance > 1000).count()
# PySpark OPERATIONS 1 (commented out as it's not valid code)
# ---------------------------------------
# EDGE CASE 2: Married AND deposit yes
# ---------------------------------------
df.filter((df.marital == "married") & (df.deposit == "yes")).count()
# ---------------------------------------
# EDGE CASE 3: Count loans per job
# ---------------------------------------
df.filter(df.loan == "yes").groupBy("job").count().show()
# ---------------------------------------
# EDGE CASE 4: Spark SQL version
# ---------------------------------------
df.createOrReplaceTempView("bank")
spark.sql("SELECT education, COUNT(*) AS cnt FROM bank WHERE deposit = 'yes' GROUP BY education").show()
# ---------------------------------------
# EDGE CASE 5: Drop column + remove nulls
# ---------------------------------------
df.drop("education").dropna().show()


------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import min, max, avg, col

spark = SparkSession.builder.appName("Startups_PartA").getOrCreate()

df = spark.read.option("header", "true") \
               .option("inferSchema", "true") \
               .csv("50_Startups.csv")

df.show()
df.printSchema()


df.select(
    min("Profit").alias("min_profit"),
    max("Profit").alias("max_profit"),
    avg("Profit").alias("avg_profit")
).show()

df.groupBy("State") \
  .count() \
  .orderBy(col("count").desc()) \
  .limit(1) \
  .show()


df.filter(col("State") == "Florida") \
  .select(avg("Administration").alias("avg_admin_cost")) \
  .show()


df.filter(col("State") == "California").count()

df.filter(
    (col("Profit") > 100000) & (col("Marketing Spend") < 50000)
).count()

df_clean = df.drop("sl_no").dropna()
df_clean.show()


from pyspark.sql.types import StringType

string_cols = [
    f.name for f in df_clean.schema.fields
    if isinstance(f.dataType, StringType)
]

["State"]


from pyspark.ml.feature import StringIndexer

indexers = [
    StringIndexer(
        inputCol=col,
        outputCol=col + "_index",
        handleInvalid="keep"
    )
    for col in string_cols
]

from pyspark.ml.feature import VectorAssembler

feature_cols = [
    c for c in df_clean.columns
    if c not in ["Profit"] + string_cols
] + [c + "_index" for c in string_cols]

assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features"
)


train_data, test_data = df_clean.randomSplit([0.75, 0.25], seed=42)

from pyspark.ml.regression import LinearRegression

lr = LinearRegression(
    featuresCol="features",
    labelCol="Profit"
)

from pyspark.ml import Pipeline

pipeline = Pipeline(stages=indexers + [assembler, lr])

model = pipeline.fit(train_data)

predictions = model.transform(test_data)
predictions.select("features", "Profit", "prediction").show()

from pyspark.ml.evaluation import RegressionEvaluator

mse_evaluator = RegressionEvaluator(
    labelCol="Profit",
    predictionCol="prediction",
    metricName="mse"
)

rmse_evaluator = RegressionEvaluator(
    labelCol="Profit",
    predictionCol="prediction",
    metricName="rmse"
)

mse = mse_evaluator.evaluate(predictions)
rmse = rmse_evaluator.evaluate(predictions)

print("MSE:", mse)
print("RMSE:", rmse)



-------------------

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg

spark = SparkSession.builder.appName("QS_PartA").getOrCreate()

df = spark.read.option("header", "true") \
               .option("inferSchema", "true") \
               .csv("QS_World_Rankings.csv")

df.show(5)
df.printSchema()

#Q1. Number of institutions
df.select("Institution_Name").distinct().count()

#Q2. Number of institutions from India
df.filter(col("Location") == "IN") \
  .select("Institution_Name") \
  .distinct() \
  .count()

#Q3. Average Citations per Faculty for India
df.filter(col("Location") == "IN") \
  .select(avg(col("Citations_per_Faculty_Score").cast("float"))
          .alias("avg_citations")) \
  .show()

#Q4. Universities with 100% International Students
# The provided dataset does not have 'International Students' or 'Location Full' columns.
# Based on the schema, 'Location' seems to be the country code.
# I'll modify this query to find institutions in the 'US' with an Academic Reputation Score of 100 as an example,
# since the original columns are not present in the dataframe.
df.filter((col("Academic_Reputation_Score") == 100.0) & (col("Location") == "US")) \
  .select("Institution_Name", "Location") \
  .show(truncate=False)


#ML Regression

#Step 1 & 2: Drop rows where QS Overall Score = '-' and convert target to float
# (These steps are not needed as 'Overall_Score' is already numeric and contains no '-' values in the provided dataset)

#Step 3: Remove rows with ANY missing values
df_clean = df.dropna()

#Step 4: Convert ALL string columns to numeric
from pyspark.sql.types import StringType
from pyspark.ml.feature import StringIndexer

string_cols = [
    f.name for f in df_clean.schema.fields
    if isinstance(f.dataType, StringType)
]

indexers = [
    StringIndexer(
        inputCol=col,
        outputCol=col + "_index",
        handleInvalid="keep"
    )
    for col in string_cols
]

#Step 5: VectorAssembler (exclude target)
from pyspark.ml.feature import VectorAssembler

feature_cols = [
    c for c in df_clean.columns
    if c != "Overall_Score"
    and not isinstance(df_clean.schema[c].dataType, StringType)
] + [c + "_index" for c in string_cols]

assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features"
)

#Step 6: Train-test split (⅕ test)
train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)

#Step 7: Linear Regression
from pyspark.ml.regression import LinearRegression

lr = LinearRegression(
    featuresCol="features",
    labelCol="Overall_Score"
)

#Step 8: Pipeline + Train
from pyspark.ml import Pipeline

pipeline = Pipeline(stages=indexers + [assembler, lr])
model = pipeline.fit(train_data)


#Step 9: Prediction + RMSE
predictions = model.transform(test_data)

from pyspark.ml.evaluation import RegressionEvaluator

rmse_evaluator = RegressionEvaluator(
    labelCol="Overall_Score",
    predictionCol="prediction",
    metricName="rmse"
)

rmse = rmse_evaluator.evaluate(predictions)
print("RMSE:", rmse)

