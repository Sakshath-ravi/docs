{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating synthetic data to match your Spark script\n",
        "data = {\n",
        "    \"age\": [32, 45, 28, 55, 39, 41, 23, 60, 31, 48],\n",
        "    \"job\": [\"admin.\", \"technician\", \"management\", \"retired\", \"blue-collar\", \"admin.\", \"student\", \"retired\", \"management\", \"unemployed\"],\n",
        "    \"marital\": [\"single\", \"married\", \"single\", \"married\", \"married\", \"divorced\", \"single\", \"married\", \"married\", \"divorced\"],\n",
        "    \"education\": [\"tertiary\", \"secondary\", \"tertiary\", \"primary\", \"secondary\", \"secondary\", \"secondary\", \"primary\", \"tertiary\", \"secondary\"],\n",
        "    \"loan\": [\"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\"],\n",
        "    \"balance\": [2500, 50, 1200, 8000, -150, 450, 10, 15000, 3200, 0],\n",
        "    \"deposit\": [\"yes\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\"]\n",
        "}\n",
        "\n",
        "df_pandas = pd.DataFrame(data)\n",
        "df_pandas.to_csv(\"bank_data.csv\", index=False)\n",
        "print(\"Dataset 'bank_data.csv' created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTzStzOmotaz",
        "outputId": "d2f90017-eb24-4cee-f568-c9690aa54a76"
      },
      "id": "LTzStzOmotaz",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'bank_data.csv' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "# Spark session\n",
        "spark = SparkSession.builder.appName(\"SparkML\").getOrCreate()\n",
        "\n",
        "# Load data\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"bank_data.csv\")\n",
        "\n",
        "# String columns\n",
        "string_cols = [\"job\", \"marital\", \"education\", \"loan\", \"deposit\"]\n",
        "\n",
        "# StringIndexers (IMPORTANT FIX)\n",
        "indexers = [\n",
        " StringIndexer(\n",
        " inputCol=col,\n",
        " outputCol=col + \"_index\",\n",
        " handleInvalid=\"keep\"\n",
        " )\n",
        " for col in string_cols\n",
        "]\n",
        "\n",
        "\n",
        "# Feature columns\n",
        "feature_cols = [\n",
        " \"age\",\n",
        " \"balance\",\n",
        " \"job_index\",\n",
        " \"marital_index\",\n",
        " \"education_index\",\n",
        " \"loan_index\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "6LCVdog2gEj0"
      },
      "id": "6LCVdog2gEj0",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VectorAssembler\n",
        "assembler = VectorAssembler(\n",
        " inputCols=feature_cols,\n",
        " outputCol=\"features\"\n",
        ")\n",
        "\n",
        "\n",
        "# Logistic Regression model\n",
        "lr = LogisticRegression(\n",
        " featuresCol=\"features\",\n",
        " labelCol=\"deposit_index\"\n",
        ")\n",
        "\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
        "\n",
        "\n",
        "# Train-test split\n",
        "train_data, test_data = df.randomSplit([0.75, 0.25], seed=42)\n",
        "\n",
        "\n",
        "# Train model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "\n",
        "# Predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        " labelCol=\"deposit_index\",\n",
        " predictionCol=\"prediction\",\n",
        " metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwKbSCwfhC3l",
        "outputId": "e9b1514a-61fc-4e55-b8a6-8fb45088294a"
      },
      "id": "kwKbSCwfhC3l",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import min, max, avg\n",
        "# 1️⃣ Create Spark Session\n",
        "spark = SparkSession.builder.appName(\"SectionC_PartA\").getOrCreate()\n",
        "# 2️⃣ Load Dataset\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"bank_data.csv\")\n",
        "# Verify data\n",
        "df.show()\n",
        "df.printSchema()\n",
        "# ---------------------------------------\n",
        "# Q1. Minimum, Maximum, Average balance\n",
        "# ---------------------------------------\n",
        "df.select(min(\"balance\").alias(\"min_balance\"), max(\"balance\").alias(\"max_balance\"), avg(\"balance\").alias(\"avg_balance\")).show()\n",
        "# ---------------------------------------\n",
        "# Q2. Job type with highest number of people\n",
        "# ---------------------------------------\n",
        "df.groupBy(\"job\").count().orderBy(\"count\", ascending=False).show()\n",
        "# Only top job\n",
        "df.groupBy(\"job\").count().orderBy(\"count\", ascending=False).limit(1).show()\n",
        "# ---------------------------------------\n",
        "# Q3. Count people aged over 60 AND having a loan\n",
        "# ---------------------------------------\n",
        "df.filter((df.age > 60) & (df.loan == \"yes\")).count()\n",
        "# ---------------------------------------\n",
        "# Q4. Count deposits ('yes') by education level\n",
        "# ---------------------------------------\n",
        "df.filter(df.deposit == \"yes\").groupBy(\"education\").count().show()\n",
        "# ---------------------------------------\n",
        "# Q5. Deposit count ('yes' vs 'no') for each marital status\n",
        "# ---------------------------------------\n",
        "df.groupBy(\"marital\", \"deposit\").count().show()\n",
        "# ---------------------------------------\n",
        "# Q6. Remove all rows containing ANY null values\n",
        "# ---------------------------------------\n",
        "df_clean = df.dropna()\n",
        "df_clean.show()\n",
        "# ---------------------------------------\n",
        "# EDGE CASE 1: Balance greater than 1000\n",
        "# ---------------------------------------\n",
        "df.filter(df.balance > 1000).count()\n",
        "# PySpark OPERATIONS 1 (commented out as it's not valid code)\n",
        "# ---------------------------------------\n",
        "# EDGE CASE 2: Married AND deposit yes\n",
        "# ---------------------------------------\n",
        "df.filter((df.marital == \"married\") & (df.deposit == \"yes\")).count()\n",
        "# ---------------------------------------\n",
        "# EDGE CASE 3: Count loans per job\n",
        "# ---------------------------------------\n",
        "df.filter(df.loan == \"yes\").groupBy(\"job\").count().show()\n",
        "# ---------------------------------------\n",
        "# EDGE CASE 4: Spark SQL version\n",
        "# ---------------------------------------\n",
        "df.createOrReplaceTempView(\"bank\")\n",
        "spark.sql(\"SELECT education, COUNT(*) AS cnt FROM bank WHERE deposit = 'yes' GROUP BY education\").show()\n",
        "# ---------------------------------------\n",
        "# EDGE CASE 5: Drop column + remove nulls\n",
        "# ---------------------------------------\n",
        "df.drop(\"education\").dropna().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYa49Nf029Ba",
        "outputId": "13d4043b-7c01-493c-d2d9-66f5af4a6a37"
      },
      "id": "UYa49Nf029Ba",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+--------+---------+----+-------+-------+\n",
            "|age|        job| marital|education|loan|balance|deposit|\n",
            "+---+-----------+--------+---------+----+-------+-------+\n",
            "| 32|     admin.|  single| tertiary|  no|   2500|    yes|\n",
            "| 45| technician| married|secondary| yes|     50|     no|\n",
            "| 28| management|  single| tertiary|  no|   1200|    yes|\n",
            "| 55|    retired| married|  primary|  no|   8000|    yes|\n",
            "| 39|blue-collar| married|secondary| yes|   -150|     no|\n",
            "| 41|     admin.|divorced|secondary|  no|    450|     no|\n",
            "| 23|    student|  single|secondary|  no|     10|    yes|\n",
            "| 60|    retired| married|  primary|  no|  15000|    yes|\n",
            "| 31| management| married| tertiary|  no|   3200|     no|\n",
            "| 48| unemployed|divorced|secondary| yes|      0|     no|\n",
            "+---+-----------+--------+---------+----+-------+-------+\n",
            "\n",
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            "\n",
            "+-----------+-----------+-----------+\n",
            "|min_balance|max_balance|avg_balance|\n",
            "+-----------+-----------+-----------+\n",
            "|       -150|      15000|     3026.0|\n",
            "+-----------+-----------+-----------+\n",
            "\n",
            "+-----------+-----+\n",
            "|        job|count|\n",
            "+-----------+-----+\n",
            "| management|    2|\n",
            "|    retired|    2|\n",
            "|     admin.|    2|\n",
            "|    student|    1|\n",
            "|blue-collar|    1|\n",
            "| technician|    1|\n",
            "| unemployed|    1|\n",
            "+-----------+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|       job|count|\n",
            "+----------+-----+\n",
            "|management|    2|\n",
            "+----------+-----+\n",
            "\n",
            "+---------+-----+\n",
            "|education|count|\n",
            "+---------+-----+\n",
            "| tertiary|    2|\n",
            "|secondary|    1|\n",
            "|  primary|    2|\n",
            "+---------+-----+\n",
            "\n",
            "+--------+-------+-----+\n",
            "| marital|deposit|count|\n",
            "+--------+-------+-----+\n",
            "|  single|    yes|    3|\n",
            "|divorced|     no|    2|\n",
            "| married|    yes|    2|\n",
            "| married|     no|    3|\n",
            "+--------+-------+-----+\n",
            "\n",
            "+---+-----------+--------+---------+----+-------+-------+\n",
            "|age|        job| marital|education|loan|balance|deposit|\n",
            "+---+-----------+--------+---------+----+-------+-------+\n",
            "| 32|     admin.|  single| tertiary|  no|   2500|    yes|\n",
            "| 45| technician| married|secondary| yes|     50|     no|\n",
            "| 28| management|  single| tertiary|  no|   1200|    yes|\n",
            "| 55|    retired| married|  primary|  no|   8000|    yes|\n",
            "| 39|blue-collar| married|secondary| yes|   -150|     no|\n",
            "| 41|     admin.|divorced|secondary|  no|    450|     no|\n",
            "| 23|    student|  single|secondary|  no|     10|    yes|\n",
            "| 60|    retired| married|  primary|  no|  15000|    yes|\n",
            "| 31| management| married| tertiary|  no|   3200|     no|\n",
            "| 48| unemployed|divorced|secondary| yes|      0|     no|\n",
            "+---+-----------+--------+---------+----+-------+-------+\n",
            "\n",
            "+-----------+-----+\n",
            "|        job|count|\n",
            "+-----------+-----+\n",
            "|blue-collar|    1|\n",
            "| technician|    1|\n",
            "| unemployed|    1|\n",
            "+-----------+-----+\n",
            "\n",
            "+---------+---+\n",
            "|education|cnt|\n",
            "+---------+---+\n",
            "| tertiary|  2|\n",
            "|secondary|  1|\n",
            "|  primary|  2|\n",
            "+---------+---+\n",
            "\n",
            "+---+-----------+--------+----+-------+-------+\n",
            "|age|        job| marital|loan|balance|deposit|\n",
            "+---+-----------+--------+----+-------+-------+\n",
            "| 32|     admin.|  single|  no|   2500|    yes|\n",
            "| 45| technician| married| yes|     50|     no|\n",
            "| 28| management|  single|  no|   1200|    yes|\n",
            "| 55|    retired| married|  no|   8000|    yes|\n",
            "| 39|blue-collar| married| yes|   -150|     no|\n",
            "| 41|     admin.|divorced|  no|    450|     no|\n",
            "| 23|    student|  single|  no|     10|    yes|\n",
            "| 60|    retired| married|  no|  15000|    yes|\n",
            "| 31| management| married|  no|   3200|     no|\n",
            "| 48| unemployed|divorced| yes|      0|     no|\n",
            "+---+-----------+--------+----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8KUQ1W29-yE1"
      },
      "id": "8KUQ1W29-yE1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"R&D Spend\": [165349.2, 162597.7, 153441.5, 144372.4, 142107.3, 131876.9, 134615.5, 130298.1],\n",
        "    \"Administration\": [136897.8, 151377.6, 101145.6, 118671.9, 91391.8, 99814.7, 147198.9, 145530.1],\n",
        "    \"Marketing Spend\": [471784.1, 443898.5, 407934.5, 383199.6, 366168.4, 362861.4, 127716.8, 323876.7],\n",
        "    \"State\": [\"New York\", \"California\", \"Florida\", \"New York\", \"Florida\", \"New York\", \"California\", \"Florida\"],\n",
        "    \"Profit\": [192261.8, 191792.1, 191050.4, 182901.9, 166187.9, 156991.1, 156122.5, 155752.6]\n",
        "}\n",
        "\n",
        "df_startups = pd.DataFrame(data)\n",
        "df_startups.to_csv(\"50_Startups.csv\", index=False)\n",
        "print(\"File '50_Startups.csv' created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxrg3aLv-x4U",
        "outputId": "b812c945-bf60-42a6-e752-4c355cf1d36f"
      },
      "id": "Cxrg3aLv-x4U",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '50_Startups.csv' created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import min, max, avg, col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Startups_PartA\").getOrCreate()\n",
        "\n",
        "df = spark.read.option(\"header\", \"true\") \\\n",
        "               .option(\"inferSchema\", \"true\") \\\n",
        "               .csv(\"50_Startups.csv\")\n",
        "\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl7N_JHZ-xor",
        "outputId": "8c7829b6-7b76-4c3e-dc52-4590373ece0d"
      },
      "id": "jl7N_JHZ-xor",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+---------------+----------+--------+\n",
            "|R&D Spend|Administration|Marketing Spend|     State|  Profit|\n",
            "+---------+--------------+---------------+----------+--------+\n",
            "| 165349.2|      136897.8|       471784.1|  New York|192261.8|\n",
            "| 162597.7|      151377.6|       443898.5|California|191792.1|\n",
            "| 153441.5|      101145.6|       407934.5|   Florida|191050.4|\n",
            "| 144372.4|      118671.9|       383199.6|  New York|182901.9|\n",
            "| 142107.3|       91391.8|       366168.4|   Florida|166187.9|\n",
            "| 131876.9|       99814.7|       362861.4|  New York|156991.1|\n",
            "| 134615.5|      147198.9|       127716.8|California|156122.5|\n",
            "| 130298.1|      145530.1|       323876.7|   Florida|155752.6|\n",
            "+---------+--------------+---------------+----------+--------+\n",
            "\n",
            "root\n",
            " |-- R&D Spend: double (nullable = true)\n",
            " |-- Administration: double (nullable = true)\n",
            " |-- Marketing Spend: double (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Profit: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\n",
        "    min(\"Profit\").alias(\"min_profit\"),\n",
        "    max(\"Profit\").alias(\"max_profit\"),\n",
        "    avg(\"Profit\").alias(\"avg_profit\")\n",
        ").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1ilPRLD_AZ-",
        "outputId": "ec0aec8f-0ed7-4b80-a978-a4ae9d185fc8"
      },
      "id": "C1ilPRLD_AZ-",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------------------+\n",
            "|min_profit|max_profit|        avg_profit|\n",
            "+----------+----------+------------------+\n",
            "|  155752.6|  192261.8|174132.53750000003|\n",
            "+----------+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"State\") \\\n",
        "  .count() \\\n",
        "  .orderBy(col(\"count\").desc()) \\\n",
        "  .limit(1) \\\n",
        "  .show()\n",
        "\n",
        "\n",
        "df.filter(col(\"State\") == \"Florida\") \\\n",
        "  .select(avg(\"Administration\").alias(\"avg_admin_cost\")) \\\n",
        "  .show()\n",
        "\n",
        "\n",
        "df.filter(col(\"State\") == \"California\").count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GikMdLvV_I69",
        "outputId": "40a59a6e-30f2-4119-e99d-e633eb2bbd99"
      },
      "id": "GikMdLvV_I69",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  State|count|\n",
            "+-------+-----+\n",
            "|Florida|    3|\n",
            "+-------+-----+\n",
            "\n",
            "+------------------+\n",
            "|    avg_admin_cost|\n",
            "+------------------+\n",
            "|112689.16666666667|\n",
            "+------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(\n",
        "    (col(\"Profit\") > 100000) & (col(\"Marketing Spend\") < 50000)\n",
        ").count()\n",
        "\n",
        "df_clean = df.drop(\"sl_no\").dropna()\n",
        "df_clean.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lichxR5t_dob",
        "outputId": "8f9d8c64-5509-4079-dd95-347f82568a1e"
      },
      "id": "lichxR5t_dob",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+---------------+----------+--------+\n",
            "|R&D Spend|Administration|Marketing Spend|     State|  Profit|\n",
            "+---------+--------------+---------------+----------+--------+\n",
            "| 165349.2|      136897.8|       471784.1|  New York|192261.8|\n",
            "| 162597.7|      151377.6|       443898.5|California|191792.1|\n",
            "| 153441.5|      101145.6|       407934.5|   Florida|191050.4|\n",
            "| 144372.4|      118671.9|       383199.6|  New York|182901.9|\n",
            "| 142107.3|       91391.8|       366168.4|   Florida|166187.9|\n",
            "| 131876.9|       99814.7|       362861.4|  New York|156991.1|\n",
            "| 134615.5|      147198.9|       127716.8|California|156122.5|\n",
            "| 130298.1|      145530.1|       323876.7|   Florida|155752.6|\n",
            "+---------+--------------+---------------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "string_cols = [\n",
        "    f.name for f in df_clean.schema.fields\n",
        "    if isinstance(f.dataType, StringType)\n",
        "]\n",
        "\n",
        "[\"State\"]\n",
        "\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(\n",
        "        inputCol=col,\n",
        "        outputCol=col + \"_index\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    for col in string_cols\n",
        "]\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_cols = [\n",
        "    c for c in df_clean.columns\n",
        "    if c not in [\"Profit\"] + string_cols\n",
        "] + [c + \"_index\" for c in string_cols]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "\n",
        "train_data, test_data = df_clean.randomSplit([0.75, 0.25], seed=42)\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"Profit\"\n",
        ")\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
        "\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "predictions = model.transform(test_data)\n",
        "predictions.select(\"features\", \"Profit\", \"prediction\").show()\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "mse_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"Profit\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"mse\"\n",
        ")\n",
        "\n",
        "rmse_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"Profit\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ")\n",
        "\n",
        "mse = mse_evaluator.evaluate(predictions)\n",
        "rmse = rmse_evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SmenJV__lvL",
        "outputId": "a4e54e80-a2df-47c9-cfb1-e0d05d563942"
      },
      "id": "-SmenJV__lvL",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+------------------+\n",
            "|            features|  Profit|        prediction|\n",
            "+--------------------+--------+------------------+\n",
            "|[134615.5,147198....|156122.5|  303260.821257418|\n",
            "|[162597.7,151377....|191792.1|213746.22997319244|\n",
            "+--------------------+--------+------------------+\n",
            "\n",
            "MSE: 11065834702.665487\n",
            "RMSE: 105194.27124451924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vI_FEaLDDHNd"
      },
      "id": "vI_FEaLDDHNd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating synthetic data matching the QS World University Rankings schema\n",
        "data = {\n",
        "    \"Rank\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    \"Institution_Name\": [\n",
        "        \"MIT\", \"Imperial College London\", \"University of Oxford\",\n",
        "        \"Harvard University\", \"University of Cambridge\", \"Stanford University\",\n",
        "        \"ETH Zurich\", \"NUS Singapore\", \"UCL\", \"Caltech\"\n",
        "    ],\n",
        "    \"Location\": [\"US\", \"UK\", \"UK\", \"US\", \"UK\", \"US\", \"CH\", \"SG\", \"UK\", \"US\"],\n",
        "    \"Academic_Reputation_Score\": [100.0, 98.5, 100.0, 100.0, 100.0, 100.0, 98.8, 99.5, 99.5, 96.5],\n",
        "    \"Employer_Reputation_Score\": [100.0, 99.5, 100.0, 100.0, 100.0, 100.0, 87.2, 91.1, 98.3, 95.3],\n",
        "    \"Faculty_Student_Score\": [100.0, 98.2, 100.0, 96.3, 100.0, 100.0, 65.9, 68.8, 95.9, 100.0],\n",
        "    \"Citations_per_Faculty_Score\": [100.0, 93.9, 84.8, 100.0, 84.6, 99.0, 97.9, 93.1, 72.2, 100.0],\n",
        "    \"Overall_Score\": [100.0, 98.5, 96.9, 96.8, 96.7, 96.1, 93.9, 93.7, 91.6, 90.9]\n",
        "}\n",
        "\n",
        "df_qs = pd.DataFrame(data)\n",
        "df_qs.to_csv(\"QS_World_Rankings.csv\", index=False)\n",
        "print(\"File 'QS_World_Rankings.csv' created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZoBTFUkDG-e",
        "outputId": "5a6195e3-088a-473c-f79d-4951c24e74f0"
      },
      "id": "PZoBTFUkDG-e",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'QS_World_Rankings.csv' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "spark = SparkSession.builder.appName(\"QS_PartA\").getOrCreate()\n",
        "\n",
        "df = spark.read.option(\"header\", \"true\") \\\n",
        "               .option(\"inferSchema\", \"true\") \\\n",
        "               .csv(\"QS_World_Rankings.csv\")\n",
        "\n",
        "df.show(5)\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRjAPeilDJOb",
        "outputId": "d96e9da6-f691-4092-f1c7-d9d958b3e548"
      },
      "id": "dRjAPeilDJOb",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+--------+-------------------------+-------------------------+---------------------+---------------------------+-------------+\n",
            "|Rank|    Institution_Name|Location|Academic_Reputation_Score|Employer_Reputation_Score|Faculty_Student_Score|Citations_per_Faculty_Score|Overall_Score|\n",
            "+----+--------------------+--------+-------------------------+-------------------------+---------------------+---------------------------+-------------+\n",
            "|   1|                 MIT|      US|                    100.0|                    100.0|                100.0|                      100.0|        100.0|\n",
            "|   2|Imperial College ...|      UK|                     98.5|                     99.5|                 98.2|                       93.9|         98.5|\n",
            "|   3|University of Oxford|      UK|                    100.0|                    100.0|                100.0|                       84.8|         96.9|\n",
            "|   4|  Harvard University|      US|                    100.0|                    100.0|                 96.3|                      100.0|         96.8|\n",
            "|   5|University of Cam...|      UK|                    100.0|                    100.0|                100.0|                       84.6|         96.7|\n",
            "+----+--------------------+--------+-------------------------+-------------------------+---------------------+---------------------------+-------------+\n",
            "only showing top 5 rows\n",
            "root\n",
            " |-- Rank: integer (nullable = true)\n",
            " |-- Institution_Name: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Academic_Reputation_Score: double (nullable = true)\n",
            " |-- Employer_Reputation_Score: double (nullable = true)\n",
            " |-- Faculty_Student_Score: double (nullable = true)\n",
            " |-- Citations_per_Faculty_Score: double (nullable = true)\n",
            " |-- Overall_Score: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1. Number of institutions\n",
        "df.select(\"Institution_Name\").distinct().count()\n",
        "\n",
        "#Q2. Number of institutions from India\n",
        "df.filter(col(\"Location\") == \"IN\") \\\n",
        "  .select(\"Institution_Name\") \\\n",
        "  .distinct() \\\n",
        "  .count()\n",
        "\n",
        "#Q3. Average Citations per Faculty for India\n",
        "df.filter(col(\"Location\") == \"IN\") \\\n",
        "  .select(avg(col(\"Citations_per_Faculty_Score\").cast(\"float\"))\n",
        "          .alias(\"avg_citations\")) \\\n",
        "  .show()\n",
        "\n",
        "#Q4. Universities with 100% International Students\n",
        "# The provided dataset does not have 'International Students' or 'Location Full' columns.\n",
        "# Based on the schema, 'Location' seems to be the country code.\n",
        "# I'll modify this query to find institutions in the 'US' with an Academic Reputation Score of 100 as an example,\n",
        "# since the original columns are not present in the dataframe.\n",
        "df.filter((col(\"Academic_Reputation_Score\") == 100.0) & (col(\"Location\") == \"US\")) \\\n",
        "  .select(\"Institution_Name\", \"Location\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yg-KrmjDMYs",
        "outputId": "d49a30d7-b6c2-4549-f0d2-28d480d93b5f"
      },
      "id": "2Yg-KrmjDMYs",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|avg_citations|\n",
            "+-------------+\n",
            "|         NULL|\n",
            "+-------------+\n",
            "\n",
            "+-------------------+--------+\n",
            "|Institution_Name   |Location|\n",
            "+-------------------+--------+\n",
            "|MIT                |US      |\n",
            "|Harvard University |US      |\n",
            "|Stanford University|US      |\n",
            "+-------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ML Regression\n",
        "\n",
        "#Step 1 & 2: Drop rows where QS Overall Score = '-' and convert target to float\n",
        "# (These steps are not needed as 'Overall_Score' is already numeric and contains no '-' values in the provided dataset)\n",
        "\n",
        "#Step 3: Remove rows with ANY missing values\n",
        "df_clean = df.dropna()\n",
        "\n",
        "#Step 4: Convert ALL string columns to numeric\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "string_cols = [\n",
        "    f.name for f in df_clean.schema.fields\n",
        "    if isinstance(f.dataType, StringType)\n",
        "]\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(\n",
        "        inputCol=col,\n",
        "        outputCol=col + \"_index\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    for col in string_cols\n",
        "]\n",
        "\n",
        "#Step 5: VectorAssembler (exclude target)\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_cols = [\n",
        "    c for c in df_clean.columns\n",
        "    if c != \"Overall_Score\"\n",
        "    and not isinstance(df_clean.schema[c].dataType, StringType)\n",
        "] + [c + \"_index\" for c in string_cols]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "#Step 6: Train-test split (⅕ test)\n",
        "train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "#Step 7: Linear Regression\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"Overall_Score\"\n",
        ")\n",
        "\n",
        "#Step 8: Pipeline + Train\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "\n",
        "#Step 9: Prediction + RMSE\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "rmse_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"Overall_Score\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ")\n",
        "\n",
        "rmse = rmse_evaluator.evaluate(predictions)\n",
        "print(\"RMSE:\", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kd9UNZBGLIE",
        "outputId": "cb0817cd-94db-4ac3-ffa4-0aceb30e1f08"
      },
      "id": "0Kd9UNZBGLIE",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 1.604505978645793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ft3MmvFNLnUw"
      },
      "id": "ft3MmvFNLnUw"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}